# main.py
# æ‰¿æ“”Agentè·è²¬
import init, inference, action
import warnings

# whisperé è¨­ä½¿ç”¨FP16(GPU CUDAä½¿ç”¨)ï¼Œé€™è£¡ä½¿ç”¨CPUé‹ç®—æœƒè·³è­¦å‘Šï¼ŒåŒæ™‚è‡ªå‹•è½‰ç‚ºFP32CPUé‹ç®—
warnings.filterwarnings("ignore", category=UserWarning, module="whisper.transcribe")

def main():
    # åˆå§‹åŒ– Whisper æ¨¡å‹
    audio_model, semantic_model, YOLO_model, action_embeddings = init.init()

    while True:
        user_input = input("\n\nâ­â­â­è«‹è¼¸å…¥æŒ‡ä»¤ (è¼¸å…¥ 'SPK' é€²å…¥èªéŸ³æ¨¡å¼ï¼Œè¼¸å…¥ 'Q' é›¢é–‹): ").strip().upper()

        if user_input == 'Q':
            print("ğŸ‘‹ ç¨‹åºçµæŸï¼Œå†è¦‹ï¼")
            break
        
        if user_input == 'SPK':
            print("ğŸ¤ é€²å…¥èªéŸ³æ¨¡å¼")
            action.record_audio("./Sound/mic_input.wav")
            input_text = inference.transcribe_audio("./Sound/mic_input.wav", audio_model)
            print(f"è½‰æ›æ–‡å­—: {input_text}")
        else:
            input_text = user_input
        if not input_text.strip():
            print("âš ï¸ è¼¸å…¥ä¸å¾—ç‚ºç©º")
            continue
        
        best_match, confidence = inference.analyze_text(input_text, semantic_model, action_embeddings)
        action.perform_action(best_match, confidence, YOLO_model, audio_model)

if __name__ == '__main__':
    main()